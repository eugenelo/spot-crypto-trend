{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00143780",
   "metadata": {},
   "source": [
    "# Crypto Momentum / Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26929d",
   "metadata": {},
   "source": [
    "This notebook includes analysis on momentum effects in Cryptocurrencies. We analyze both cross-sectional and time-series momentum, and look for possibile methods of monetizing these effects.\n",
    "\n",
    "Our dataset comes from trading history for pairs on the Kraken exchange ([link](https://support.kraken.com/hc/en-us/articles/360047543791-Downloadable-historical-market-data-time-and-sales-)). We further constrain our analysis to USD pairs only. Kraken is our primary trading exchange due to geographical restrictions (US...zzz) and Kraken's superior fee structure as compared to other US exchanges like Coinbase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e1865",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd143f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pathlib import Path\n",
    "from numba import njit, jit\n",
    "import vectorbt as vbt\n",
    "\n",
    "from analysis.analysis import analysis\n",
    "\n",
    "from simulation.simulation import simulation\n",
    "from simulation.backtest import backtest_crypto\n",
    "from simulation.optimize import optimize_crypto\n",
    "from signal_generation.signal_generation import (\n",
    "    create_analysis_signals,\n",
    "    create_trading_signals,\n",
    ")\n",
    "from signal_generation.constants import SignalType\n",
    "from signal_generation.rohrbach import create_rohrbach_signals\n",
    "from signal_generation.common import (\n",
    "    ema,\n",
    "    volatility_ema,\n",
    "    bins,\n",
    "    volatility,\n",
    "    future_volatility,\n",
    ")\n",
    "from position_generation.benchmark import generate_benchmark_btc\n",
    "from position_generation.constants import (\n",
    "    NUM_LONG_ASSETS_COL,\n",
    "    NUM_SHORT_ASSETS_COL,\n",
    "    NUM_KEPT_ASSETS_COL,\n",
    "    SCALED_POSITION_COL,\n",
    "    NUM_OPEN_LONG_POSITIONS_COL,\n",
    "    NUM_OPEN_SHORT_POSITIONS_COL,\n",
    "    NUM_OPEN_POSITIONS_COL,\n",
    ")\n",
    "from position_generation.utils import nonempty_positions\n",
    "from position_generation.generate_positions import generate_positions\n",
    "from position_generation.utils import Direction\n",
    "from data.utils import load_ohlc_to_hourly_filtered, load_ohlc_to_daily_filtered\n",
    "from core.utils import filter_universe\n",
    "from core.constants import (\n",
    "    in_universe_excl_stablecoins,\n",
    "    in_shitcoin_trending_universe,\n",
    "    in_mature_trending_universe,\n",
    ")\n",
    "\n",
    "np.set_printoptions(linewidth=1000)\n",
    "pd.set_option(\"display.width\", 2000)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/home/elo/data/usd_ohlc_fixed.csv\"\n",
    "input_freq = \"1h\"\n",
    "start_date = \"2020/01/01\"\n",
    "end_date = \"2023/12/31\"\n",
    "tz = pytz.timezone(\"UTC\")\n",
    "start_date = tz.localize(datetime.strptime(start_date.replace(\"/\", \"-\"), \"%Y-%m-%d\"))\n",
    "end_date = tz.localize(datetime.strptime(end_date.replace(\"/\", \"-\"), \"%Y-%m-%d\"))\n",
    "\n",
    "# Parse data\n",
    "df_daily = load_ohlc_to_daily_filtered(\n",
    "    input_path, input_freq=input_freq, tz=tz, whitelist_fn=in_universe_excl_stablecoins\n",
    ")\n",
    "\n",
    "# Create signals\n",
    "df_analysis = create_analysis_signals(df_daily, periods_per_day=1)\n",
    "\n",
    "# Validate dates\n",
    "data_start = df_analysis[\"timestamp\"].min()\n",
    "if start_date < data_start:\n",
    "    print(f\"Input start_date is before start of data! Setting to {data_start}\")\n",
    "    start_date = data_start\n",
    "data_end = df_analysis[\"timestamp\"].max()\n",
    "if end_date > data_end:\n",
    "    print(f\"Input end_date is after end of data! Setting to {data_end}\")\n",
    "    end_date = data_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a58ee8",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce19ba",
   "metadata": {},
   "source": [
    "### Introductory Analysis - Past Returns vs Future Returns\n",
    "\n",
    "Let's try the simplest, dumbest thing first. Look for a relationship between past (30d) returns and future (14-28d) returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis(\n",
    "    df_analysis,\n",
    "    feature=\"30d_log_returns\",\n",
    "    target=\"next_7d_log_returns\",\n",
    "    bin_feature=\"30d_log_returns_decile\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis(\n",
    "    df_analysis,\n",
    "    feature=\"30d_log_returns\",\n",
    "    target=\"next_14d_log_returns\",\n",
    "    bin_feature=\"30d_log_returns_decile\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7dee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis(\n",
    "    df_analysis,\n",
    "    feature=\"30d_log_returns\",\n",
    "    target=\"next_28d_log_returns\",\n",
    "    bin_feature=\"30d_log_returns_decile\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c206eac",
   "metadata": {},
   "source": [
    "We see evidence of momentum effects using 30 day past returns. The explanatory power of 30d returns seems to increase as the future lookahead horizon increases (28d > 14d > 7d).\n",
    "\n",
    "The effect seems to be quite noisy year to year. Is there a relationship between momentum and whether the market was going up/down? Let's use BTC as a proxy for the market and look at returns per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0016ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get years where BTC returns were positive\n",
    "df_tmp = (\n",
    "    df_analysis.loc[df_analysis[\"ticker\"] == \"BTC/USD\"]\n",
    "    .groupby([\"year\", \"ticker\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"returns\": \"sum\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index(0)\n",
    ")\n",
    "df_tmp[\"up_year\"] = df_tmp[\"returns\"] > 0\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze momentum effects for up years\n",
    "up_years = [2013, 2015, 2016, 2017, 2019, 2020, 2021, 2023]\n",
    "\n",
    "feature = \"30d_log_returns\"\n",
    "bin_feature = \"30d_log_returns_decile\"\n",
    "target = \"next_28d_log_returns\"\n",
    "\n",
    "# Plot de-meaned future returns over 30d return deciles per year\n",
    "for year in up_years:\n",
    "    df_tmp = (\n",
    "        df_analysis.loc[df_analysis[\"year\"] == year]\n",
    "        .groupby(\n",
    "            [\n",
    "                bin_feature,\n",
    "            ]\n",
    "        )\n",
    "        .agg({target: \"mean\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    # De-mean\n",
    "    df_tmp[target] = df_tmp[target] - df_tmp[target].mean()\n",
    "    # Plot\n",
    "    fig = px.bar(\n",
    "        df_tmp,\n",
    "        x=bin_feature,\n",
    "        y=target,\n",
    "        title=year,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4590c4",
   "metadata": {},
   "source": [
    "There are still some exceptions (2015, 2020), but overall the effect seems to persist. The exceptional years support the notion that harnessing this effect is somewhat shitty, and so is likely to persist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f477b3",
   "metadata": {},
   "source": [
    "### Effects of Volume on Relationship\n",
    "\n",
    "Our hypothesis for why momentum exists includes both behavioral reasons (FOMO, flows yolo-ing into coins going up) as well as limits to arbitrage (kinda risky/shitty to take the other side of such a volatile trade). If this is true, we would expect to see the relationship strengthen for the really shitty shitcoins (where limits to arbitrage are greater due to limited capacity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6636d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "df_effect_vs_volume = pd.DataFrame(\n",
    "    {\n",
    "        \"max_dollar_volume\": pd.Series(dtype=\"int\"),\n",
    "        \"slope\": pd.Series(dtype=\"float\"),\n",
    "        \"r2\": pd.Series(dtype=\"float\"),\n",
    "        \"num_data_points\": pd.Series(dtype=\"int\"),\n",
    "    }\n",
    ")\n",
    "feature = \"30d_log_returns\"\n",
    "target = \"next_28d_log_returns\"\n",
    "bin_feature = \"30d_log_returns_decile\"\n",
    "for dollar_volume in [\n",
    "    np.inf,\n",
    "    100e6,\n",
    "    10e6,\n",
    "    1e6,\n",
    "    500000,\n",
    "    100000,\n",
    "    50000,\n",
    "    40000,\n",
    "    30000,\n",
    "    20000,\n",
    "    10000,\n",
    "    5000,\n",
    "    2000,\n",
    "    1000,\n",
    "    100,\n",
    "]:\n",
    "    volume_mask = (df_analysis[\"dollar_volume\"] <= dollar_volume) & (\n",
    "        df_analysis[\"dollar_volume\"] > 0\n",
    "    )\n",
    "    df_tmp = df_analysis.loc[volume_mask].dropna()\n",
    "    # Linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "        df_tmp[feature], df_tmp[target]\n",
    "    )\n",
    "    num_data_points = len(df_tmp)\n",
    "    df_effect_vs_volume.loc[len(df_effect_vs_volume.index)] = [\n",
    "        dollar_volume,\n",
    "        slope,\n",
    "        r_value**2,\n",
    "        num_data_points,\n",
    "    ]\n",
    "\n",
    "df_effect_vs_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da9ec45",
   "metadata": {},
   "source": [
    "The data seems to support this hypothesis up to a max daily volume of ~\\$10000.\n",
    "\n",
    "A follow-up question: is this driven by some shitcoin outliers? Let's plot the scatterplot for the low dollar volume datapoints to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"30d_log_returns\"\n",
    "target = \"next_28d_log_returns\"\n",
    "bin_feature = \"30d_log_returns_decile\"\n",
    "volume_mask = (df_analysis[\"dollar_volume\"] <= 20000) & (\n",
    "    df_analysis[\"dollar_volume\"] > 0\n",
    ")\n",
    "analysis(\n",
    "    df_analysis.loc[volume_mask],\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfab80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print outliers\n",
    "df_analysis.dropna().loc[volume_mask].sort_values(\n",
    "    by=\"next_28d_log_returns\", ascending=False\n",
    ")[\n",
    "    [\n",
    "        \"ticker\",\n",
    "        \"timestamp\",\n",
    "        \"volume\",\n",
    "        \"dollar_volume\",\n",
    "        \"30d_returns\",\n",
    "        \"next_28d_log_returns\",\n",
    "    ]\n",
    "].head(\n",
    "    50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e13fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers from analysis\n",
    "feature = \"30d_log_returns\"\n",
    "target = \"next_28d_log_returns\"\n",
    "bin_feature = \"30d_log_returns_decile\"\n",
    "analysis(\n",
    "    df_analysis.loc[volume_mask & (df_analysis[\"next_28d_log_returns\"] <= 2.0)],\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e8553",
   "metadata": {},
   "source": [
    "Even after we filter out the egregiously high returns (> 100% over the next month), the relationship seems to hold up.\n",
    "\n",
    "At this point, we can be reasonably confident of the following:\n",
    "- Momentum effects exist in cryptocurrency markets\n",
    "- Effects seem to strengthen inversely proportional to daily traded volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab2be22",
   "metadata": {},
   "source": [
    "### Trend Overextension\n",
    "\n",
    "There's a phenomenon known as \"trend overextension\" which describes the fact that very large signals of trend may actually predict reversion in future returns (due to the trend being overextended, more capital is willing to take the other side of the trade to bring prices back down to \"fair value\").\n",
    "\n",
    "Do we see this in our cryptocurrency data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82cc543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create centiles\n",
    "bin_feature = \"30d_log_returns_centile\"\n",
    "df_analysis[bin_feature] = bins(df_analysis, column=\"30d_log_returns\", num_bins=100)\n",
    "# Create rank\n",
    "rank_feature = \"30d_log_returns_rank\"\n",
    "df_analysis[rank_feature] = (\n",
    "    df_analysis.groupby([\"timestamp\"])[\"30d_log_returns\"]\n",
    "    .fillna(value=0)\n",
    "    .rank(method=\"dense\", ascending=False)\n",
    "    .astype(int)\n",
    ")\n",
    "# Remove outliers\n",
    "df_analysis_filtered = df_analysis.loc[df_analysis[\"next_28d_log_returns\"] < 2.0]\n",
    "\n",
    "\n",
    "feature = rank_feature\n",
    "target = \"next_28d_log_returns\"\n",
    "\n",
    "# All Data\n",
    "df_tmp = (\n",
    "    df_analysis_filtered.groupby([rank_feature]).agg({target: \"mean\"}).reset_index()\n",
    ")\n",
    "fig = px.bar(df_tmp, x=rank_feature, y=target, title=\"All Data\")\n",
    "fig.show()\n",
    "\n",
    "# # Low Volume\n",
    "# df_tmp = (\n",
    "#     df_analysis_filtered.loc[df_analysis[\"dollar_volume\"] <= 10000]\n",
    "#     .groupby([rank_feature])\n",
    "#     .agg({target: \"mean\"})\n",
    "#     .reset_index()\n",
    "# )\n",
    "# fig = px.bar(df_tmp, x=rank_feature, y=target, title=\"Dollar Volume <= $10,000\")\n",
    "# fig.show()\n",
    "\n",
    "# # High Volume\n",
    "# df_tmp = (\n",
    "#     df_analysis_filtered.loc[df_analysis[\"dollar_volume\"] >= 1000000]\n",
    "#     .groupby([rank_feature])\n",
    "#     .agg({target: \"mean\"})\n",
    "#     .reset_index()\n",
    "# )\n",
    "# fig = px.bar(df_tmp, x=rank_feature, y=target, title=\"Dollar Volume >= $1,000,000\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c00944",
   "metadata": {},
   "source": [
    "I can't really say I see evidence of overextension to be honest...I thought I had seen it in a previous analysis but I can't really reproduce. I mean, there's maybe some evidence beyond the top ~10% but then it shoots back up in the last 4%.\n",
    "\n",
    "There's maybe some evidence for it in the low volume tickers.\n",
    "\n",
    "This is relevant for deciding which activation function to use (sigmoid vs $x * exp(-x^2)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3dfd0",
   "metadata": {},
   "source": [
    "## Trend Signal (Rohrbach et. al 2017)\n",
    "\n",
    "Rohrbach and coauthors published a 2017 paper titled \"Momentum and trend following trading strategies for currencies and bitcoin\" ([link](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2949379)) in which they describe a formula for a trend signal. Let's take a look at how well this predicts returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68382cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Rohrbach signal is already generated under \"trend_signal\"\n",
    "df_analysis = create_rohrbach_signals(df_analysis, periods_per_day=1)\n",
    "\n",
    "feature = \"trend_signal\"\n",
    "target = \"next_28d_log_returns\"\n",
    "bin_feature = \"trend_decile\"\n",
    "analysis(\n",
    "    df_analysis,\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146f8d9",
   "metadata": {},
   "source": [
    "The shape of the decile (cross-sectional) bar plots look roughly the same as the plots using 30d returns, which is a good sign. In the scatterplot, the r_value is only marginally higher but the plot's outliers look a lot better (more up and to the right).\n",
    "\n",
    "How about the volume-filtered analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf90d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trend_vs_volume = pd.DataFrame(\n",
    "    {\n",
    "        \"max_dollar_volume\": pd.Series(dtype=\"int\"),\n",
    "        \"slope\": pd.Series(dtype=\"float\"),\n",
    "        \"r2\": pd.Series(dtype=\"float\"),\n",
    "        \"num_data_points\": pd.Series(dtype=\"int\"),\n",
    "    }\n",
    ")\n",
    "feature = \"trend_signal\"\n",
    "target = \"next_28d_log_returns\"\n",
    "bin_feature = \"trend_decile\"\n",
    "for dollar_volume in [\n",
    "    np.inf,\n",
    "    100e6,\n",
    "    10e6,\n",
    "    1e6,\n",
    "    500000,\n",
    "    100000,\n",
    "    50000,\n",
    "    40000,\n",
    "    30000,\n",
    "    20000,\n",
    "    10000,\n",
    "    5000,\n",
    "    2000,\n",
    "    1000,\n",
    "    100,\n",
    "]:\n",
    "    volume_mask = (df_analysis[\"dollar_volume\"] <= dollar_volume) & (\n",
    "        df_analysis[\"dollar_volume\"] > 0\n",
    "    )\n",
    "    df_tmp = df_analysis.loc[volume_mask].dropna()\n",
    "    # Linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "        df_tmp[feature], df_tmp[target]\n",
    "    )\n",
    "    num_data_points = len(df_tmp)\n",
    "    df_trend_vs_volume.loc[len(df_trend_vs_volume.index)] = [\n",
    "        dollar_volume,\n",
    "        slope,\n",
    "        r_value**2,\n",
    "        num_data_points,\n",
    "    ]\n",
    "\n",
    "df_trend_vs_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"trend_signal\"\n",
    "target = \"next_28d_log_returns\"\n",
    "bin_feature = \"trend_decile\"\n",
    "volume_mask = (df_analysis[\"dollar_volume\"] <= 100000) & (\n",
    "    df_analysis[\"dollar_volume\"] > 0\n",
    ")\n",
    "analysis(\n",
    "    df_analysis.loc[volume_mask],\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c8858",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter outliers from analysis\n",
    "feature = \"trend_signal\"\n",
    "target = \"next_28d_log_returns\"\n",
    "bin_feature = \"trend_decile\"\n",
    "analysis(\n",
    "    df_analysis.loc[volume_mask & (df_analysis[\"next_28d_log_returns\"] <= 2.0)],\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a60e1",
   "metadata": {},
   "source": [
    "As before, effects are stronger as volume decreases (although with this signal, caps out around \\$1 - \\$10k daily volume). Effects persist even after removing egregious outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b317ef50",
   "metadata": {},
   "source": [
    "What happens with the higher volume data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce768fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at higher volume data only\n",
    "feature = \"trend_signal\"\n",
    "target = \"next_28d_log_returns\"\n",
    "bin_feature = \"trend_decile\"\n",
    "volume_mask = (df_analysis[\"dollar_volume\"] >= 1e6) & (df_analysis[\"dollar_volume\"] > 0)\n",
    "analysis(\n",
    "    df_analysis.loc[volume_mask],\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfcce2d",
   "metadata": {},
   "source": [
    "Despite the r_value being smaller, we see this hugely obvious positive linear relationship at the extremes. What are these data? Is this a special case/one-off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3eb4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna().loc[volume_mask].sort_values(\n",
    "    by=\"next_28d_log_returns\", ascending=False\n",
    ")[\n",
    "    [\"ticker\", \"timestamp\", \"dollar_volume\", \"trend_signal\", \"next_28d_log_returns\"]\n",
    "].head(\n",
    "    50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30987691",
   "metadata": {},
   "source": [
    "It's the great DOGE pump of 2021 as well as a pump in 2017 (unaware). What happens if we remove these outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers from analysis\n",
    "feature = \"trend_signal\"\n",
    "target = \"next_28d_log_returns\"\n",
    "bin_feature = \"trend_decile\"\n",
    "analysis(\n",
    "    df_analysis.loc[volume_mask & (df_analysis[\"next_28d_log_returns\"] <= 2.0)],\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20256cc5",
   "metadata": {},
   "source": [
    "We still see a generally positive relationship between trend and future returns even with the outliers removed.\n",
    "\n",
    "At this point, we can probably state the following:\n",
    "- Trend effects are stronger on average in lower volume tickers (up to a certain point).\n",
    "- High volume tickers contain some of the most egregiously high trend <-> returns data points. This may align with our hypothesis for where trends come from: in order to see an egregiously strong trend, you need investors to ape into the same coin (which will drive up volumes).\n",
    "- We want to target the low volume universe due to stronger trend effects on average, but also want to include the high volume universe to capture any future manias (also trend is still generally present in high volume)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c57ad9d",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30faeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis[\"trend_sigmoid_decile\"] = bins(\n",
    "    df_analysis, column=\"trend_signal_sigmoid\", num_bins=10\n",
    ")\n",
    "\n",
    "# The Rohrbach signal w/ sigmoid activation is already generated under \"trend_signal_sigmoid\"\n",
    "feature = \"trend_signal_sigmoid\"\n",
    "target = \"next_28d_log_returns\"\n",
    "bin_feature = \"trend_sigmoid_decile\"\n",
    "analysis(\n",
    "    df_analysis,\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56ddb6",
   "metadata": {},
   "source": [
    "Sigmoid seems marginally better than exponential, at least in data analysis. Correlation is roughly the same (as expected, both are just transformations of the same signal), but slope is ~2x higher.\n",
    "\n",
    "Does the answer change as a function of volume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trend_vs_volume = pd.DataFrame(\n",
    "    {\n",
    "        \"max_dollar_volume\": pd.Series(dtype=\"int\"),\n",
    "        \"slope_exponential\": pd.Series(dtype=\"float\"),\n",
    "        \"r2_exponential\": pd.Series(dtype=\"float\"),\n",
    "        \"slope_sigmoid\": pd.Series(dtype=\"float\"),\n",
    "        \"r2_sigmoid\": pd.Series(dtype=\"float\"),\n",
    "        \"num_data_points\": pd.Series(dtype=\"int\"),\n",
    "    }\n",
    ")\n",
    "feature_exponential = \"trend_signal\"\n",
    "feature_sigmoid = \"trend_signal_sigmoid\"\n",
    "target = \"next_28d_log_returns\"\n",
    "for dollar_volume in [\n",
    "    np.inf,\n",
    "    100e6,\n",
    "    10e6,\n",
    "    1e6,\n",
    "    500000,\n",
    "    100000,\n",
    "    50000,\n",
    "    40000,\n",
    "    30000,\n",
    "    20000,\n",
    "    10000,\n",
    "    5000,\n",
    "    2000,\n",
    "    1000,\n",
    "    100,\n",
    "]:\n",
    "    volume_mask = (df_analysis[\"dollar_volume\"] <= dollar_volume) & (\n",
    "        df_analysis[\"dollar_volume\"] > 0\n",
    "    )\n",
    "    df_tmp = df_analysis.loc[volume_mask].dropna()\n",
    "    # Linear regression\n",
    "    (\n",
    "        slope_exponential,\n",
    "        intercept,\n",
    "        r_value_exponential,\n",
    "        p_value,\n",
    "        std_err,\n",
    "    ) = stats.linregress(df_tmp[feature_exponential], df_tmp[target])\n",
    "    slope_sigmoid, intercept, r_value_sigmoid, p_value, std_err = stats.linregress(\n",
    "        df_tmp[feature_sigmoid], df_tmp[target]\n",
    "    )\n",
    "    num_data_points = len(df_tmp)\n",
    "    df_trend_vs_volume.loc[len(df_trend_vs_volume.index)] = [\n",
    "        dollar_volume,\n",
    "        slope_exponential,\n",
    "        r_value_exponential**2,\n",
    "        slope_sigmoid,\n",
    "        r_value_sigmoid**2,\n",
    "        num_data_points,\n",
    "    ]\n",
    "\n",
    "df_trend_vs_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d080532",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The Rohrbach signal w/ sigmoid activation is already generated under \"trend_signal_sigmoid\"\n",
    "target = \"next_28d_log_returns\"\n",
    "bin_feature_exponential = \"trend_decile\"\n",
    "bin_feature_sigmoid = \"trend_sigmoid_decile\"\n",
    "\n",
    "# All Data\n",
    "df_tmp = (\n",
    "    df_analysis.groupby([bin_feature_exponential]).agg({target: \"mean\"}).reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_tmp, x=bin_feature_exponential, y=target, title=\"All Data - Exponential\"\n",
    ")\n",
    "fig.show()\n",
    "df_tmp = df_analysis.groupby([bin_feature_sigmoid]).agg({target: \"mean\"}).reset_index()\n",
    "fig = px.bar(df_tmp, x=bin_feature_sigmoid, y=target, title=\"All Data - Sigmoid\")\n",
    "fig.show()\n",
    "\n",
    "# Low Volume\n",
    "df_tmp = (\n",
    "    df_analysis.loc[\n",
    "        (df_analysis[\"dollar_volume\"] <= 100000) & (df_analysis[\"dollar_volume\"] > 0)\n",
    "    ]\n",
    "    .groupby([bin_feature_exponential])\n",
    "    .agg({target: \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_tmp,\n",
    "    x=bin_feature_exponential,\n",
    "    y=target,\n",
    "    title=\"Dollar Volume <= $100,000 - Exponential\",\n",
    ")\n",
    "fig.show()\n",
    "df_tmp = (\n",
    "    df_analysis.loc[\n",
    "        (df_analysis[\"dollar_volume\"] <= 100000) & (df_analysis[\"dollar_volume\"] > 0)\n",
    "    ]\n",
    "    .groupby([bin_feature_sigmoid])\n",
    "    .agg({target: \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_tmp, x=bin_feature_sigmoid, y=target, title=\"Dollar Volume <= $100,000 - Sigmoid\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# High Volume\n",
    "df_tmp = (\n",
    "    df_analysis.loc[df_analysis[\"dollar_volume\"] >= 1000000]\n",
    "    .groupby([bin_feature_exponential])\n",
    "    .agg({target: \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_tmp,\n",
    "    x=bin_feature_exponential,\n",
    "    y=target,\n",
    "    title=\"Dollar Volume >= $1,000,000 - Exponential\",\n",
    ")\n",
    "fig.show()\n",
    "df_tmp = (\n",
    "    df_analysis.loc[df_analysis[\"dollar_volume\"] >= 1000000]\n",
    "    .groupby([bin_feature_sigmoid])\n",
    "    .agg({target: \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_tmp,\n",
    "    x=bin_feature_sigmoid,\n",
    "    y=target,\n",
    "    title=\"Dollar Volume >= $1,000,000 - Sigmoid\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333bf94",
   "metadata": {},
   "source": [
    "Sigmoid just seems better at every level of volume.\n",
    "\n",
    "**Takeaways:**\n",
    "- **Use sigmoid activation function**\n",
    "- **If incorporating cross-sectional momentum,**\n",
    "  - **For low volume universe keep top 3 deciles (top 30%)**\n",
    "  - **For high volume universe keep top 2 deciles (top 15-20%)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcbeb36",
   "metadata": {},
   "source": [
    "## Combined Model - Multivariate OLS\n",
    "\n",
    "What other factors can we combine with trend to predict returns? Some ideas I've seen mentioned in other places include: funding rates, basis, borrowing rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb88423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# Rename column starting with number\n",
    "df_analysis[\"thirty_day_returns\"] = df_analysis[\"30d_returns\"]\n",
    "df_analysis[\"thirty_day_dollar_volume\"] = df_analysis[\"30d_dollar_volume\"]\n",
    "\n",
    "features = [\"thirty_day_returns\", \"thirty_day_dollar_volume\"]\n",
    "target = \"next_28d_returns\"\n",
    "result = sm.ols(\n",
    "    formula=f\"{target} ~ {' + '.join(features)}\", data=df_analysis.dropna()\n",
    ").fit()\n",
    "print(result.summary())\n",
    "\n",
    "print(result.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f75532",
   "metadata": {},
   "source": [
    "# Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df7447",
   "metadata": {},
   "source": [
    "### Trend Signal Rolling Absolute Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930aac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Trend Signal Mean: {df_analysis[\"trend_signal\"].dropna().mean():.2f}')\n",
    "print(\n",
    "    f'Positive Trend Signal Mean: {df_analysis.loc[df_analysis[\"trend_signal\"] >= 0][\"trend_signal\"].dropna().mean():.2f}'\n",
    ")\n",
    "print(\n",
    "    f'Negative Trend Signal Mean: {df_analysis.loc[df_analysis[\"trend_signal\"] < 0][\"trend_signal\"].dropna().mean():.2f}'\n",
    ")\n",
    "print(\n",
    "    f'Absolute Trend Signal Mean: {np.abs(df_analysis[\"trend_signal\"].dropna()).mean():.2f}'\n",
    ")\n",
    "print(f'Trend Signal Median: {df_analysis[\"trend_signal\"].dropna().median():.2f}')\n",
    "fig = px.histogram(df_analysis, x=\"trend_signal\")\n",
    "fig.show()\n",
    "\n",
    "df_analysis[\"abs_trend_signal\"] = np.abs(df_analysis[\"trend_signal\"])\n",
    "df_tmp = (\n",
    "    df_analysis.groupby([\"timestamp\"]).agg({\"abs_trend_signal\": \"mean\"}).reset_index()\n",
    ")\n",
    "df_tmp[\"abs_trend_signal_30d_ema\"] = (\n",
    "    df_tmp[\"abs_trend_signal\"].ewm(span=180, adjust=True, ignore_na=False).mean()\n",
    ")\n",
    "fig = px.line(df_tmp, x=\"timestamp\", y=\"abs_trend_signal_30d_ema\")\n",
    "fig.show()\n",
    "\n",
    "df_positive_only = df_analysis.loc[df_analysis[\"trend_signal\"] > 0]\n",
    "df_tmp = (\n",
    "    df_positive_only.groupby(\"timestamp\").agg({\"trend_signal\": \"mean\"}).reset_index()\n",
    ")\n",
    "df_tmp[\"pos_trend_signal_30d_ema\"] = (\n",
    "    df_tmp[\"trend_signal\"].ewm(span=180, adjust=True, ignore_na=False).mean()\n",
    ")\n",
    "fig = px.line(df_tmp, x=\"timestamp\", y=\"pos_trend_signal_30d_ema\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Trend Signal Mean: {df_analysis[\"trend_signal_sigmoid\"].dropna().mean():.2f}')\n",
    "print(\n",
    "    f'Positive Trend Signal Mean: {df_analysis.loc[df_analysis[\"trend_signal_sigmoid\"] >= 0][\"trend_signal_sigmoid\"].mean():.2f}'\n",
    ")\n",
    "print(\n",
    "    f'Negative Trend Signal Mean: {df_analysis.loc[df_analysis[\"trend_signal_sigmoid\"] < 0][\"trend_signal_sigmoid\"].mean():.2f}'\n",
    ")\n",
    "print(\n",
    "    f'Absolute Trend Signal Mean: {np.abs(df_analysis[\"trend_signal_sigmoid\"].dropna()).mean():.2f}'\n",
    ")\n",
    "print(\n",
    "    f'Trend Signal Median: {df_analysis[\"trend_signal_sigmoid\"].dropna().median():.2f}'\n",
    ")\n",
    "fig = px.histogram(df_analysis, x=\"trend_signal_sigmoid\")\n",
    "fig.show()\n",
    "\n",
    "df_analysis[\"abs_trend_signal_sigmoid\"] = np.abs(df_analysis[\"trend_signal_sigmoid\"])\n",
    "df_tmp = (\n",
    "    df_analysis.groupby([\"timestamp\"])\n",
    "    .agg({\"abs_trend_signal_sigmoid\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df_tmp[\"abs_trend_signal_sigmoid_30d_ema\"] = (\n",
    "    df_tmp[\"abs_trend_signal_sigmoid\"]\n",
    "    .ewm(span=180, adjust=True, ignore_na=False)\n",
    "    .mean()\n",
    ")\n",
    "fig = px.line(df_tmp, x=\"timestamp\", y=\"abs_trend_signal_sigmoid_30d_ema\")\n",
    "fig.show()\n",
    "\n",
    "df_positive_only = df_analysis.loc[df_analysis[\"trend_signal_sigmoid\"] > 0]\n",
    "df_tmp = (\n",
    "    df_positive_only.groupby(\"timestamp\")\n",
    "    .agg({\"trend_signal_sigmoid\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df_tmp[\"pos_trend_signal_sigmoid_30d_ema\"] = (\n",
    "    df_tmp[\"trend_signal_sigmoid\"].ewm(span=180, adjust=True, ignore_na=False).mean()\n",
    ")\n",
    "fig = px.line(df_tmp, x=\"timestamp\", y=\"pos_trend_signal_sigmoid_30d_ema\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958fd2a",
   "metadata": {},
   "source": [
    "### Number of Open Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = \"trend_signal\"\n",
    "periods_per_day = 1\n",
    "direction = Direction.LongOnly\n",
    "vol_target = 0.35\n",
    "cross_sectional_percentage = 0.2\n",
    "min_daily_volume = None\n",
    "max_daily_volume = None\n",
    "\n",
    "df_analysis = create_trading_signals(\n",
    "    df_daily, periods_per_day=periods_per_day, signal_type=SignalType.Rohrbach\n",
    ")\n",
    "df_positions = generate_positions(\n",
    "    df_analysis,\n",
    "    signal=signal,\n",
    "    periods_per_day=periods_per_day,\n",
    "    direction=direction,\n",
    "    vol_target=vol_target,\n",
    "    cross_sectional_percentage=cross_sectional_percentage,\n",
    "    min_daily_volume=min_daily_volume,\n",
    "    max_daily_volume=max_daily_volume,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_positions_mask = ~df_positions[\"scaled_position\"].isna()\n",
    "print(df_positions.loc[valid_positions_mask][\"scaled_position\"].max())\n",
    "\n",
    "df_tmp = (\n",
    "    df_positions.groupby([\"timestamp\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"scaled_position\": \"sum\",\n",
    "            NUM_OPEN_LONG_POSITIONS_COL: \"first\",\n",
    "            NUM_OPEN_SHORT_POSITIONS_COL: \"first\",\n",
    "            NUM_OPEN_POSITIONS_COL: \"first\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.line(df_tmp, x=\"timestamp\", y=[\"scaled_position\"])\n",
    "fig.show()\n",
    "fig = px.line(\n",
    "    df_tmp,\n",
    "    x=\"timestamp\",\n",
    "    y=[\n",
    "        NUM_OPEN_LONG_POSITIONS_COL,\n",
    "        NUM_OPEN_SHORT_POSITIONS_COL,\n",
    "        NUM_OPEN_POSITIONS_COL,\n",
    "    ],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707dcb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
