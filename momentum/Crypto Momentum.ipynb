{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00143780",
   "metadata": {},
   "source": [
    "# Crypto Momentum / Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26929d",
   "metadata": {},
   "source": [
    "This notebook includes analysis on momentum effects in Cryptocurrencies. We analyze both cross-sectional and time-series momentum, and look for possibile methods of monetizing these effects.\n",
    "\n",
    "Our dataset comes from trading history for pairs on the Kraken exchange ([link](https://support.kraken.com/hc/en-us/articles/360047543791-Downloadable-historical-market-data-time-and-sales-)). We further constrain our analysis to USD pairs only. Kraken is our primary trading exchange due to geographical restrictions (US...zzz) and Kraken's superior fee structure as compared to other US exchanges like Coinbase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e1865",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd143f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pathlib import Path\n",
    "from numba import njit, jit\n",
    "from typing import List\n",
    "\n",
    "from analysis.analysis import analysis\n",
    "\n",
    "from simulation.vbt import *\n",
    "from simulation.simulation import *\n",
    "from simulation.backtest import *\n",
    "from simulation.stats import *\n",
    "from signal_generation.signal_generation import *\n",
    "from signal_generation.constants import *\n",
    "from signal_generation.rohrbach import *\n",
    "from signal_generation.common import *\n",
    "from position_generation.benchmark import *\n",
    "from position_generation.constants import *\n",
    "from position_generation.utils import *\n",
    "from position_generation.generate_positions import *\n",
    "from position_generation.utils import *\n",
    "from data.utils import *\n",
    "from core.utils import *\n",
    "from core.constants import *\n",
    "\n",
    "np.set_printoptions(linewidth=1000)\n",
    "pd.set_option(\"display.width\", 2000)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/home/elo/data/usd_ohlc_fixed.csv\"\n",
    "input_freq = \"1h\"\n",
    "start_date = \"2014/01/01\"\n",
    "end_date = \"2023/12/31\"\n",
    "tz = pytz.timezone(\"UTC\")\n",
    "start_date = tz.localize(datetime.strptime(start_date.replace(\"/\", \"-\"), \"%Y-%m-%d\"))\n",
    "end_date = tz.localize(datetime.strptime(end_date.replace(\"/\", \"-\"), \"%Y-%m-%d\"))\n",
    "\n",
    "# Parse data\n",
    "df_daily = load_ohlc_to_daily_filtered(\n",
    "    input_path, input_freq=input_freq, tz=tz, whitelist_fn=in_universe_excl_stablecoins\n",
    ")\n",
    "\n",
    "# Create signals\n",
    "df_analysis = create_analysis_signals(df_daily, periods_per_day=1)\n",
    "\n",
    "# Validate dates\n",
    "data_start = df_analysis[\"timestamp\"].min()\n",
    "if start_date < data_start:\n",
    "    print(f\"Input start_date is before start of data! Setting to {data_start}\")\n",
    "    start_date = data_start\n",
    "data_end = df_analysis[\"timestamp\"].max()\n",
    "if end_date > data_end:\n",
    "    print(f\"Input end_date is after end of data! Setting to {data_end}\")\n",
    "    end_date = data_end\n",
    "\n",
    "# Exclude 2013 and older\n",
    "df_analysis = df_analysis.loc[df_analysis[\"timestamp\"] >= start_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a58ee8",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce19ba",
   "metadata": {},
   "source": [
    "### Introductory Analysis - Past Returns vs Future Returns\n",
    "\n",
    "Let's try the simplest, dumbest thing first. Look for a relationship between past (30d) returns and future (14-28d) returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis(\n",
    "    df_analysis,\n",
    "    feature=\"15d_log_returns\",\n",
    "    target=\"next_1d_log_returns\",\n",
    "    bin_feature=\"15d_log_returns_decile\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c206eac",
   "metadata": {},
   "source": [
    "We see some slight evidence of momentum effects using 15 day past returns.\n",
    "\n",
    "The effect seems to be quite noisy year to year. Is there a relationship between momentum and whether the market was going up/down? Let's use BTC as a proxy for the market and look at returns per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0016ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get years where BTC returns were positive\n",
    "df_tmp = (\n",
    "    df_analysis.loc[df_analysis[\"ticker\"] == \"BTC/USD\"]\n",
    "    .groupby([\"year\", \"ticker\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"returns\": \"sum\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index(0)\n",
    ")\n",
    "df_tmp[\"up_year\"] = df_tmp[\"returns\"] > 0\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze momentum effects for up years\n",
    "up_years = [2015, 2016, 2017, 2019, 2020, 2021, 2023]\n",
    "\n",
    "feature = \"15d_log_returns\"\n",
    "bin_feature = \"15d_log_returns_decile\"\n",
    "target = \"next_1d_log_returns\"\n",
    "\n",
    "# Plot de-meaned future returns over 30d return deciles per year\n",
    "for year in up_years:\n",
    "    df_tmp = (\n",
    "        df_analysis.loc[df_analysis[\"year\"] == year]\n",
    "        .groupby(\n",
    "            [\n",
    "                bin_feature,\n",
    "            ]\n",
    "        )\n",
    "        .agg({target: \"mean\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    # De-mean\n",
    "    df_tmp[target] = df_tmp[target] - df_tmp[target].mean()\n",
    "    # Plot\n",
    "    fig = px.bar(\n",
    "        df_tmp,\n",
    "        x=bin_feature,\n",
    "        y=target,\n",
    "        title=year,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4590c4",
   "metadata": {},
   "source": [
    "There are still some exceptions (2015, 2020), but overall the effect seems to persist. The exceptional years support the notion that harnessing this effect is somewhat shitty, and so is likely to persist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f477b3",
   "metadata": {},
   "source": [
    "### Effects of Volume on Relationship\n",
    "\n",
    "Our hypothesis for why momentum exists includes both behavioral reasons (FOMO, flows yolo-ing into coins going up) as well as limits to arbitrage (kinda risky/shitty to take the other side of such a volatile trade). If this is true, we would expect to see the relationship strengthen for the really shitty shitcoins (where limits to arbitrage are greater due to limited capacity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6636d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "df_effect_vs_volume = pd.DataFrame(\n",
    "    {\n",
    "        \"max_dollar_volume\": pd.Series(dtype=\"int\"),\n",
    "        \"slope\": pd.Series(dtype=\"float\"),\n",
    "        \"r2\": pd.Series(dtype=\"float\"),\n",
    "        \"num_data_points\": pd.Series(dtype=\"int\"),\n",
    "    }\n",
    ")\n",
    "feature = \"30d_log_returns\"\n",
    "target = \"next_1d_log_returns\"\n",
    "bin_feature = \"30d_log_returns_decile\"\n",
    "for dollar_volume in [\n",
    "    np.inf,\n",
    "    100e6,\n",
    "    10e6,\n",
    "    5e6,\n",
    "    1e6,\n",
    "    500000,\n",
    "    400000,\n",
    "    300000,\n",
    "    200000,\n",
    "    100000,\n",
    "    50000,\n",
    "    40000,\n",
    "    30000,\n",
    "    20000,\n",
    "    10000,\n",
    "    5000,\n",
    "    2000,\n",
    "    1000,\n",
    "    100,\n",
    "]:\n",
    "    volume_mask = (df_analysis[\"dollar_volume\"] <= dollar_volume) & (\n",
    "        df_analysis[\"dollar_volume\"] > 0\n",
    "    )\n",
    "    df_tmp = df_analysis.loc[volume_mask].dropna()\n",
    "    # Linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "        df_tmp[feature], df_tmp[target]\n",
    "    )\n",
    "    num_data_points = len(df_tmp)\n",
    "    df_effect_vs_volume.loc[len(df_effect_vs_volume.index)] = [\n",
    "        dollar_volume,\n",
    "        slope,\n",
    "        r_value**2,\n",
    "        num_data_points,\n",
    "    ]\n",
    "\n",
    "df_effect_vs_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da9ec45",
   "metadata": {},
   "source": [
    "I don't see this supported in the data at all, actually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce4dda",
   "metadata": {},
   "source": [
    "#### Prior analysis, back when shitcoins seemed to be a better opportunity\n",
    "\n",
    "A follow-up question: is this driven by some shitcoin outliers? Let's plot the scatterplot for the low dollar volume datapoints to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"15d_log_returns\"\n",
    "target = \"next_1d_log_returns\"\n",
    "bin_feature = \"15d_log_returns_decile\"\n",
    "volume_mask = (df_analysis[\"dollar_volume\"] <= 100000) & (\n",
    "    df_analysis[\"dollar_volume\"] > 5000\n",
    ")\n",
    "analysis(\n",
    "    df_analysis.loc[volume_mask],\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfab80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print outliers\n",
    "df_analysis.dropna().loc[volume_mask].sort_values(\n",
    "    by=\"next_1d_log_returns\", ascending=False\n",
    ")[\n",
    "    [\n",
    "        \"ticker\",\n",
    "        \"timestamp\",\n",
    "        \"volume\",\n",
    "        \"dollar_volume\",\n",
    "        \"30d_returns\",\n",
    "        \"15d_returns\",\n",
    "        \"next_1d_log_returns\",\n",
    "    ]\n",
    "].head(\n",
    "    50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e13fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers from analysis\n",
    "feature = \"15d_log_returns\"\n",
    "target = \"next_1d_log_returns\"\n",
    "bin_feature = \"15d_log_returns_decile\"\n",
    "analysis(\n",
    "    df_analysis.loc[\n",
    "        volume_mask\n",
    "        & (np.abs(df_analysis[target]) <= 2.0)\n",
    "        & (np.abs(df_analysis[feature]) <= 2.0)\n",
    "    ],\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e8553",
   "metadata": {},
   "source": [
    "Even after we filter out the egregiously high returns (both historical and future), the relationship seems to hold up.\n",
    "\n",
    "At this point, we can be reasonably confident of the following:\n",
    "- Momentum effects exist in cryptocurrency markets\n",
    "- (Harnessable) Effects do not seem to strengthen inversely proportional to daily traded volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab2be22",
   "metadata": {},
   "source": [
    "### Trend Overextension\n",
    "\n",
    "There's a phenomenon known as \"trend overextension\" which describes the fact that very large signals of trend may actually predict reversion in future returns (due to the trend being overextended, more capital is willing to take the other side of the trade to bring prices back down to \"fair value\").\n",
    "\n",
    "Do we see this in our cryptocurrency data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82cc543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create centiles\n",
    "bin_feature = \"15d_log_returns_centile\"\n",
    "df_analysis[bin_feature] = bins(df_analysis, column=\"30d_log_returns\", num_bins=50)\n",
    "# Remove outliers\n",
    "df_analysis_filtered = df_analysis.loc[df_analysis[\"next_1d_log_returns\"] < 2.0]\n",
    "target = \"next_1d_log_returns\"\n",
    "\n",
    "# All Data\n",
    "df_tmp = df_analysis_filtered.groupby([bin_feature]).agg({target: \"mean\"}).reset_index()\n",
    "fig = px.bar(df_tmp, x=bin_feature, y=target, title=\"All Data\")\n",
    "fig.show()\n",
    "\n",
    "# Low Volume\n",
    "df_tmp = (\n",
    "    df_analysis_filtered.loc[df_analysis[\"dollar_volume\"] <= 10000]\n",
    "    .groupby([bin_feature])\n",
    "    .agg({target: \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(df_tmp, x=bin_feature, y=target, title=\"Dollar Volume <= $10,000\")\n",
    "fig.show()\n",
    "\n",
    "# High Volume\n",
    "df_tmp = (\n",
    "    df_analysis_filtered.loc[df_analysis[\"dollar_volume\"] >= 1000000]\n",
    "    .groupby([bin_feature])\n",
    "    .agg({target: \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(df_tmp, x=bin_feature, y=target, title=\"Dollar Volume >= $1,000,000\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c00944",
   "metadata": {},
   "source": [
    "I can't really say I see evidence of overextension to be honest...I thought I had seen it in a previous analysis but I can't really reproduce. I mean, there's maybe some evidence beyond the top ~10% but then it shoots back up in the last 4%.\n",
    "\n",
    "There's maybe some evidence for it in the low volume tickers.\n",
    "\n",
    "This is relevant for deciding which activation function to use (sigmoid vs $x * exp(-x^2)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3dfd0",
   "metadata": {},
   "source": [
    "## Trend Signal (Rohrbach et. al 2017)\n",
    "\n",
    "Rohrbach and coauthors published a 2017 paper titled \"Momentum and trend following trading strategies for currencies and bitcoin\" ([link](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2949379)) in which they describe a formula for a trend signal. Let's take a look at how well this predicts returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68382cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Rohrbach signal is generated under \"trend_signal\"\n",
    "df_analysis = create_rohrbach_signals(df_analysis, periods_per_day=1)\n",
    "\n",
    "feature = \"trend_signal\"\n",
    "target = \"next_1d_log_returns\"\n",
    "bin_feature = \"trend_decile\"\n",
    "analysis(\n",
    "    df_analysis,\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146f8d9",
   "metadata": {},
   "source": [
    "The shape of the decile (cross-sectional) bar plots look roughly the same as the plots using 30d returns, which is a good sign. In the scatterplot, the r_value is only marginally higher but the plot's outliers look a lot better (more up and to the right).\n",
    "\n",
    "How about the volume-filtered analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf90d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trend_vs_volume = pd.DataFrame(\n",
    "    {\n",
    "        \"max_dollar_volume\": pd.Series(dtype=\"int\"),\n",
    "        \"slope\": pd.Series(dtype=\"float\"),\n",
    "        \"r2\": pd.Series(dtype=\"float\"),\n",
    "        \"num_data_points\": pd.Series(dtype=\"int\"),\n",
    "    }\n",
    ")\n",
    "feature = \"trend_signal\"\n",
    "target = \"next_1d_log_returns\"\n",
    "bin_feature = \"trend_decile\"\n",
    "for dollar_volume in [\n",
    "    np.inf,\n",
    "    100e6,\n",
    "    10e6,\n",
    "    5e6,\n",
    "    1e6,\n",
    "    500000,\n",
    "    400000,\n",
    "    300000,\n",
    "    200000,\n",
    "    100000,\n",
    "    50000,\n",
    "    40000,\n",
    "    30000,\n",
    "    20000,\n",
    "    10000,\n",
    "    5000,\n",
    "    2000,\n",
    "    1000,\n",
    "    100,\n",
    "]:\n",
    "    volume_mask = (df_analysis[\"dollar_volume\"] <= dollar_volume) & (\n",
    "        df_analysis[\"dollar_volume\"] > 0\n",
    "    )\n",
    "    df_tmp = df_analysis.loc[volume_mask].dropna()\n",
    "    # Linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "        df_tmp[feature], df_tmp[target]\n",
    "    )\n",
    "    num_data_points = len(df_tmp)\n",
    "    df_trend_vs_volume.loc[len(df_trend_vs_volume.index)] = [\n",
    "        dollar_volume,\n",
    "        slope,\n",
    "        r_value**2,\n",
    "        num_data_points,\n",
    "    ]\n",
    "\n",
    "df_trend_vs_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"trend_signal\"\n",
    "target = \"next_1d_log_returns\"\n",
    "bin_feature = \"trend_decile\"\n",
    "volume_mask = (df_analysis[\"dollar_volume\"] <= 100000) & (\n",
    "    df_analysis[\"dollar_volume\"] > 10000\n",
    ")\n",
    "analysis(\n",
    "    df_analysis.loc[volume_mask],\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c8858",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter outliers from analysis\n",
    "feature = \"trend_signal\"\n",
    "target = \"next_1d_log_returns\"\n",
    "bin_feature = \"trend_decile\"\n",
    "analysis(\n",
    "    df_analysis.loc[\n",
    "        volume_mask\n",
    "        & (np.abs(df_analysis[target]) <= 2.0)\n",
    "        & (np.abs(df_analysis[feature]) <= 2.0)\n",
    "    ],\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a60e1",
   "metadata": {},
   "source": [
    "As before, effects are weaker as volume decreases (and go negative around \\$10k daily volume). Rough shape of relationship persist even after removing egregious outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b317ef50",
   "metadata": {},
   "source": [
    "What happens with the higher volume data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce768fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at higher volume data only\n",
    "feature = \"trend_signal\"\n",
    "target = \"next_1d_log_returns\"\n",
    "bin_feature = \"trend_decile\"\n",
    "volume_mask = (df_analysis[\"dollar_volume\"] >= 1e6) & (df_analysis[\"dollar_volume\"] > 0)\n",
    "analysis(\n",
    "    df_analysis.loc[volume_mask],\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfcce2d",
   "metadata": {},
   "source": [
    "Returns are more strongly/positively correlated with the top decile, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20256cc5",
   "metadata": {},
   "source": [
    "At this point, we can probably state the following:\n",
    "- Trend effects are weaker on average in lower volume tickers (and actualy go negative beyond \\$10k daily volume).\n",
    "- We want to include as many \"mature\" assets as possible, excluding anything below \\$10k to \\$1M daily volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c57ad9d",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30faeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis[\"trend_sigmoid_decile\"] = bins(\n",
    "    df_analysis, column=\"trend_signal_sigmoid\", num_bins=10\n",
    ")\n",
    "\n",
    "# The Rohrbach signal w/ sigmoid activation is already generated under \"trend_signal_sigmoid\"\n",
    "feature = \"trend_signal_sigmoid\"\n",
    "target = \"next_1d_log_returns\"\n",
    "bin_feature = \"trend_sigmoid_decile\"\n",
    "analysis(\n",
    "    df_analysis,\n",
    "    feature=feature,\n",
    "    target=target,\n",
    "    bin_feature=bin_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56ddb6",
   "metadata": {},
   "source": [
    "Sigmoid seems more or less the same as exponential, at least in data analysis. Correlation and slope are both roughly the same (as expected, both are just transformations of the same signal).\n",
    "\n",
    "Does the answer change as a function of volume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trend_vs_volume = pd.DataFrame(\n",
    "    {\n",
    "        \"max_dollar_volume\": pd.Series(dtype=\"int\"),\n",
    "        \"slope_exponential\": pd.Series(dtype=\"float\"),\n",
    "        \"r2_exponential\": pd.Series(dtype=\"float\"),\n",
    "        \"slope_sigmoid\": pd.Series(dtype=\"float\"),\n",
    "        \"r2_sigmoid\": pd.Series(dtype=\"float\"),\n",
    "        \"num_data_points\": pd.Series(dtype=\"int\"),\n",
    "    }\n",
    ")\n",
    "feature_exponential = \"trend_signal\"\n",
    "feature_sigmoid = \"trend_signal_sigmoid\"\n",
    "target = \"next_1d_log_returns\"\n",
    "for dollar_volume in [\n",
    "    np.inf,\n",
    "    100e6,\n",
    "    10e6,\n",
    "    1e6,\n",
    "    500000,\n",
    "    400000,\n",
    "    300000,\n",
    "    200000,\n",
    "    100000,\n",
    "    50000,\n",
    "    40000,\n",
    "    30000,\n",
    "    20000,\n",
    "    10000,\n",
    "    5000,\n",
    "    2000,\n",
    "    1000,\n",
    "    100,\n",
    "]:\n",
    "    volume_mask = (df_analysis[\"dollar_volume\"] <= dollar_volume) & (\n",
    "        df_analysis[\"dollar_volume\"] > 0\n",
    "    )\n",
    "    df_tmp = df_analysis.loc[volume_mask].dropna()\n",
    "    # Linear regression\n",
    "    (\n",
    "        slope_exponential,\n",
    "        intercept,\n",
    "        r_value_exponential,\n",
    "        p_value,\n",
    "        std_err,\n",
    "    ) = stats.linregress(df_tmp[feature_exponential], df_tmp[target])\n",
    "    slope_sigmoid, intercept, r_value_sigmoid, p_value, std_err = stats.linregress(\n",
    "        df_tmp[feature_sigmoid], df_tmp[target]\n",
    "    )\n",
    "    num_data_points = len(df_tmp)\n",
    "    df_trend_vs_volume.loc[len(df_trend_vs_volume.index)] = [\n",
    "        dollar_volume,\n",
    "        slope_exponential,\n",
    "        r_value_exponential**2,\n",
    "        slope_sigmoid,\n",
    "        r_value_sigmoid**2,\n",
    "        num_data_points,\n",
    "    ]\n",
    "\n",
    "df_trend_vs_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d080532",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The Rohrbach signal w/ sigmoid activation is already generated under \"trend_signal_sigmoid\"\n",
    "target = \"next_1d_log_returns\"\n",
    "bin_feature_exponential = \"trend_decile\"\n",
    "bin_feature_sigmoid = \"trend_sigmoid_decile\"\n",
    "\n",
    "# All Data\n",
    "df_tmp = (\n",
    "    df_analysis.groupby([bin_feature_exponential]).agg({target: \"mean\"}).reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_tmp, x=bin_feature_exponential, y=target, title=\"All Data - Exponential\"\n",
    ")\n",
    "fig.show()\n",
    "df_tmp = df_analysis.groupby([bin_feature_sigmoid]).agg({target: \"mean\"}).reset_index()\n",
    "fig = px.bar(df_tmp, x=bin_feature_sigmoid, y=target, title=\"All Data - Sigmoid\")\n",
    "fig.show()\n",
    "\n",
    "# Low Volume\n",
    "low_volume_mask = (df_analysis[\"dollar_volume\"] <= 100000) & (\n",
    "    df_analysis[\"dollar_volume\"] > 5000\n",
    ")\n",
    "df_tmp = (\n",
    "    df_analysis.loc[low_volume_mask]\n",
    "    .groupby([bin_feature_exponential])\n",
    "    .agg({target: \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_tmp,\n",
    "    x=bin_feature_exponential,\n",
    "    y=target,\n",
    "    title=\"Dollar Volume <= $100,000 - Exponential\",\n",
    ")\n",
    "fig.show()\n",
    "df_tmp = (\n",
    "    df_analysis.loc[low_volume_mask]\n",
    "    .groupby([bin_feature_sigmoid])\n",
    "    .agg({target: \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_tmp, x=bin_feature_sigmoid, y=target, title=\"Dollar Volume <= $100,000 - Sigmoid\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# High Volume\n",
    "high_volume_mask = df_analysis[\"dollar_volume\"] >= 5000000\n",
    "df_tmp = (\n",
    "    df_analysis.loc[high_volume_mask]\n",
    "    .groupby([bin_feature_exponential])\n",
    "    .agg({target: \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_tmp,\n",
    "    x=bin_feature_exponential,\n",
    "    y=target,\n",
    "    title=\"Dollar Volume >= $5,000,000 - Exponential\",\n",
    ")\n",
    "fig.show()\n",
    "df_tmp = (\n",
    "    df_analysis.loc[high_volume_mask]\n",
    "    .groupby([bin_feature_sigmoid])\n",
    "    .agg({target: \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    df_tmp,\n",
    "    x=bin_feature_sigmoid,\n",
    "    y=target,\n",
    "    title=\"Dollar Volume >= $5,000,000 - Sigmoid\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333bf94",
   "metadata": {},
   "source": [
    "It really seems like a wash between these two signals.\n",
    "\n",
    "**Takeaways:**\n",
    "- **Backtest both exponential and sigmoid activation functions**\n",
    "- **If incorporating cross-sectional momentum,**\n",
    "  - **For low volume universe keep top 3 deciles (top 30%)**\n",
    "  - **For high volume universe keep top 2 deciles (top 15-20%)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcbeb36",
   "metadata": {},
   "source": [
    "## Combined Model - Multivariate OLS\n",
    "\n",
    "What other factors can we combine with trend to predict returns? Some ideas I've seen mentioned in other places include: funding rates, basis, borrowing rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb88423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# Rename column starting with number\n",
    "df_analysis[\"thirty_day_returns\"] = df_analysis[\"30d_returns\"]\n",
    "df_analysis[\"thirty_day_dollar_volume\"] = df_analysis[\"30d_dollar_volume\"]\n",
    "\n",
    "features = [\"thirty_day_returns\", \"thirty_day_dollar_volume\"]\n",
    "target = \"next_1d_returns\"\n",
    "result = sm.ols(\n",
    "    formula=f\"{target} ~ {' + '.join(features)}\", data=df_analysis.dropna()\n",
    ").fit()\n",
    "print(result.summary())\n",
    "\n",
    "print(result.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f75532",
   "metadata": {},
   "source": [
    "# Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df7447",
   "metadata": {},
   "source": [
    "### Trend Signal Rolling Absolute Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930aac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Trend Signal Mean: {df_analysis[\"trend_signal\"].dropna().mean():.2f}')\n",
    "print(\n",
    "    f'Positive Trend Signal Mean: {df_analysis.loc[df_analysis[\"trend_signal\"] >= 0][\"trend_signal\"].dropna().mean():.2f}'\n",
    ")\n",
    "print(\n",
    "    f'Negative Trend Signal Mean: {df_analysis.loc[df_analysis[\"trend_signal\"] < 0][\"trend_signal\"].dropna().mean():.2f}'\n",
    ")\n",
    "print(\n",
    "    f'Absolute Trend Signal Mean: {np.abs(df_analysis[\"trend_signal\"].dropna()).mean():.2f}'\n",
    ")\n",
    "print(f'Trend Signal Median: {df_analysis[\"trend_signal\"].dropna().median():.2f}')\n",
    "fig = px.histogram(df_analysis, x=\"trend_signal\")\n",
    "fig.show()\n",
    "\n",
    "df_analysis[\"abs_trend_signal\"] = np.abs(df_analysis[\"trend_signal\"])\n",
    "df_tmp = (\n",
    "    df_analysis.groupby([\"timestamp\"]).agg({\"abs_trend_signal\": \"mean\"}).reset_index()\n",
    ")\n",
    "df_tmp[\"abs_trend_signal_30d_ema\"] = (\n",
    "    df_tmp[\"abs_trend_signal\"].ewm(span=180, adjust=True, ignore_na=False).mean()\n",
    ")\n",
    "fig = px.line(df_tmp, x=\"timestamp\", y=\"abs_trend_signal_30d_ema\")\n",
    "fig.show()\n",
    "\n",
    "df_positive_only = df_analysis.loc[df_analysis[\"trend_signal\"] > 0]\n",
    "df_tmp = (\n",
    "    df_positive_only.groupby(\"timestamp\").agg({\"trend_signal\": \"mean\"}).reset_index()\n",
    ")\n",
    "df_tmp[\"pos_trend_signal_30d_ema\"] = (\n",
    "    df_tmp[\"trend_signal\"].ewm(span=180, adjust=True, ignore_na=False).mean()\n",
    ")\n",
    "fig = px.line(df_tmp, x=\"timestamp\", y=\"pos_trend_signal_30d_ema\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Trend Signal Mean: {df_analysis[\"trend_signal_sigmoid\"].dropna().mean():.2f}')\n",
    "print(\n",
    "    f'Positive Trend Signal Mean: {df_analysis.loc[df_analysis[\"trend_signal_sigmoid\"] >= 0][\"trend_signal_sigmoid\"].mean():.2f}'\n",
    ")\n",
    "print(\n",
    "    f'Negative Trend Signal Mean: {df_analysis.loc[df_analysis[\"trend_signal_sigmoid\"] < 0][\"trend_signal_sigmoid\"].mean():.2f}'\n",
    ")\n",
    "print(\n",
    "    f'Absolute Trend Signal Mean: {np.abs(df_analysis[\"trend_signal_sigmoid\"].dropna()).mean():.2f}'\n",
    ")\n",
    "print(\n",
    "    f'Trend Signal Median: {df_analysis[\"trend_signal_sigmoid\"].dropna().median():.2f}'\n",
    ")\n",
    "fig = px.histogram(df_analysis, x=\"trend_signal_sigmoid\")\n",
    "fig.show()\n",
    "\n",
    "df_analysis[\"abs_trend_signal_sigmoid\"] = np.abs(df_analysis[\"trend_signal_sigmoid\"])\n",
    "df_tmp = (\n",
    "    df_analysis.groupby([\"timestamp\"])\n",
    "    .agg({\"abs_trend_signal_sigmoid\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df_tmp[\"abs_trend_signal_sigmoid_30d_ema\"] = (\n",
    "    df_tmp[\"abs_trend_signal_sigmoid\"]\n",
    "    .ewm(span=180, adjust=True, ignore_na=False)\n",
    "    .mean()\n",
    ")\n",
    "fig = px.line(df_tmp, x=\"timestamp\", y=\"abs_trend_signal_sigmoid_30d_ema\")\n",
    "fig.show()\n",
    "\n",
    "df_positive_only = df_analysis.loc[df_analysis[\"trend_signal_sigmoid\"] > 0]\n",
    "df_tmp = (\n",
    "    df_positive_only.groupby(\"timestamp\")\n",
    "    .agg({\"trend_signal_sigmoid\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df_tmp[\"pos_trend_signal_sigmoid_30d_ema\"] = (\n",
    "    df_tmp[\"trend_signal_sigmoid\"].ewm(span=180, adjust=True, ignore_na=False).mean()\n",
    ")\n",
    "fig = px.line(df_tmp, x=\"timestamp\", y=\"pos_trend_signal_sigmoid_30d_ema\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958fd2a",
   "metadata": {},
   "source": [
    "### Number of Open Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = \"trend_signal\"\n",
    "periods_per_day = 1\n",
    "direction = Direction.LongOnly\n",
    "volatility_target = 0.35\n",
    "cross_sectional_percentage = None\n",
    "cross_sectional_equal_weight = False\n",
    "min_daily_volume = 10000  # Minimum avg daily volume [USD]\n",
    "max_daily_volume = None  # Maximum avg daily volume [USD]\n",
    "leverage = 5\n",
    "\n",
    "df_analysis = create_trading_signals(\n",
    "    df_daily, periods_per_day=periods_per_day, signal_type=SignalType.Rohrbach\n",
    ")\n",
    "df_positions = generate_positions(\n",
    "    df_analysis,\n",
    "    signal=signal,\n",
    "    periods_per_day=periods_per_day,\n",
    "    direction=direction,\n",
    "    volatility_target=volatility_target,\n",
    "    cross_sectional_percentage=cross_sectional_percentage,\n",
    "    cross_sectional_equal_weight=cross_sectional_equal_weight,\n",
    "    min_daily_volume=min_daily_volume,\n",
    "    max_daily_volume=max_daily_volume,\n",
    "    leverage=leverage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = (\n",
    "    get_num_open_positions(df_positions)\n",
    "    .groupby([TIMESTAMP_COL])\n",
    "    .agg(\n",
    "        {\n",
    "            NUM_LONG_ASSETS_COL: \"max\",\n",
    "            NUM_SHORT_ASSETS_COL: \"max\",\n",
    "            NUM_UNIQUE_ASSETS_COL: \"max\",\n",
    "            NUM_OPEN_LONG_POSITIONS_COL: \"max\",\n",
    "            NUM_OPEN_SHORT_POSITIONS_COL: \"max\",\n",
    "            NUM_OPEN_POSITIONS_COL: \"max\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.line(\n",
    "    df_tmp,\n",
    "    x=TIMESTAMP_COL,\n",
    "    y=[\n",
    "        NUM_LONG_ASSETS_COL,\n",
    "        NUM_SHORT_ASSETS_COL,\n",
    "        NUM_UNIQUE_ASSETS_COL,\n",
    "        NUM_OPEN_LONG_POSITIONS_COL,\n",
    "        NUM_OPEN_SHORT_POSITIONS_COL,\n",
    "        NUM_OPEN_POSITIONS_COL,\n",
    "    ],\n",
    "    title=\"Num Open Positions\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707dcb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_capital = 12000\n",
    "rebalancing_freq = None\n",
    "volume_max_size = 0.01\n",
    "rebalancing_buffer = 0.001\n",
    "\n",
    "pf_portfolio = backtest(\n",
    "    df_positions,\n",
    "    periods_per_day=periods_per_day,\n",
    "    initial_capital=initial_capital,\n",
    "    rebalancing_freq=rebalancing_freq,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    with_fees=True,\n",
    "    volume_max_size=volume_max_size,\n",
    "    rebalancing_buffer=rebalancing_buffer,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_volume = get_trade_volume(pf_portfolio)\n",
    "idx = pd.date_range(trade_volume.index.min(), trade_volume.index.max())\n",
    "trade_volume = trade_volume.reindex(idx, fill_value=0)\n",
    "\n",
    "fig = px.line(trade_volume)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83853d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover = get_turnover(pf_portfolio)\n",
    "fig = px.line(turnover)\n",
    "fig.show()\n",
    "\n",
    "turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69daff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positions.loc[\n",
    "    (df_positions[\"timestamp\"] == \"2023-01-01\") & (df_positions[\"scaled_position\"] > 0)\n",
    "][[\"ticker\", \"timestamp\", \"scaled_position\"]].sort_values(\n",
    "    by=[\"timestamp\", \"scaled_position\"], ascending=[True, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(pf_portfolio.gross_exposure)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d13fff5",
   "metadata": {},
   "source": [
    "### Inspect IDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dbeb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "idm_ser = compute_idm(df_positions, feature_column=PAST_7D_RETURNS_COL)\n",
    "idm_30d_ema = idm_ser.ewm(span=30, adjust=True, ignore_na=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = (\n",
    "    df_positions.groupby(\"timestamp\")\n",
    "    .agg({\"idm\": \"first\", \"idm_30d_ema\": \"first\"})\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.line(df_tmp, x=\"timestamp\", y=[\"idm\", \"idm_30d_ema\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b45202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idm_trend_ser = compute_idm(df_positions, feature_column=\"trend_signal\")\n",
    "idm_trend_30d_ema = idm_trend_ser.ewm(span=30, adjust=True, ignore_na=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idm_product_ser = idm_ser * idm_trend_ser\n",
    "idm_product_30d_ema = idm_product_ser.ewm(span=30, adjust=True, ignore_na=False).mean()\n",
    "# idm_product_30d_ema = idm_30d_ema * idm_trend_30d_ema\n",
    "\n",
    "df_tmp = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"idm\": idm_ser,\n",
    "            \"idm_30d_ema\": idm_30d_ema,\n",
    "            \"idm_trend\": idm_trend_ser,\n",
    "            \"idm_trend_30d_ema\": idm_trend_30d_ema,\n",
    "            \"idm_product\": idm_product_ser,\n",
    "            \"idm_product_30d_ema\": idm_product_30d_ema,\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"timestamp\"})\n",
    ")\n",
    "fig = px.line(\n",
    "    df_tmp,\n",
    "    x=\"timestamp\",\n",
    "    y=[\n",
    "        \"idm\",\n",
    "        \"idm_30d_ema\",\n",
    "        \"idm_trend\",\n",
    "        \"idm_trend_30d_ema\",\n",
    "        #         \"idm_product\",\n",
    "        #         \"idm_product_30d_ema\",\n",
    "    ],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d3d95",
   "metadata": {},
   "source": [
    "### Inspect FDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286eab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fdm(df: pd.DataFrame, feature_columns: List[str]) -> pd.Series:\n",
    "    # For the FDM, we pool signals for each ticker before computing\n",
    "    # the correlation matrix between signals (not between tickers)\n",
    "    feature_df = df[[TIMESTAMP_COL, TICKER_COL, SCALED_SIGNAL_COL] + feature_columns]\n",
    "    full_date_range = df.sort_values(by=TIMESTAMP_COL, ascending=True)[\n",
    "        TIMESTAMP_COL\n",
    "    ].unique()\n",
    "    last_stamp_updated = None\n",
    "    fdm_lst = []\n",
    "    for timestamp in full_date_range:\n",
    "        timestamp_mask = df[TIMESTAMP_COL] == timestamp\n",
    "        valid_signal_mask = ~df[SCALED_SIGNAL_COL].isna()\n",
    "        tickers = df.loc[(timestamp_mask) & (valid_signal_mask)][TICKER_COL]\n",
    "        if tickers.shape[0] == 0:\n",
    "            # No valid positions\n",
    "            fdm_lst.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        if (\n",
    "            last_stamp_updated is None\n",
    "            or (timestamp - last_stamp_updated) > IDM_REFRESH_PERIOD\n",
    "        ):\n",
    "            # Equal weight across all assets (risk parity)\n",
    "            weights = np.full(len(feature_columns), fill_value=1 / len(feature_columns))\n",
    "            # Replace negative correlations with 0\n",
    "            corr = (\n",
    "                feature_df.loc[\n",
    "                    (feature_df[TIMESTAMP_COL] <= timestamp)\n",
    "                    & (feature_df[TICKER_COL].isin(tickers))\n",
    "                ][feature_columns]\n",
    "                .corr()\n",
    "                .fillna(value=0)\n",
    "                .to_numpy()\n",
    "            )\n",
    "            corr = np.clip(corr, 0, 1)\n",
    "            fdm_divisor = np.sqrt(weights.dot(corr).dot(weights.T))\n",
    "            fdm = 1 / fdm_divisor if fdm_divisor > 0 else np.nan\n",
    "            last_stamp_updated = timestamp\n",
    "        else:\n",
    "            # Reuse previous fdm\n",
    "            fdm = fdm_lst[-1]\n",
    "        fdm_lst.append(fdm)\n",
    "    fdm_ser = pd.Series(fdm_lst, index=full_date_range, name=FDM_COL)\n",
    "    return fdm_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [f\"z_{k}\" for k in range(5)]\n",
    "fdm_ser = compute_fdm(df_positions, feature_columns=feature_columns)\n",
    "fdm_30d_ema = fdm_ser.ewm(span=30, adjust=True, ignore_na=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"fdm\": fdm_ser,\n",
    "            \"fdm_30d_ema\": fdm_30d_ema,\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"timestamp\"})\n",
    ")\n",
    "fig = px.line(\n",
    "    df_tmp,\n",
    "    x=\"timestamp\",\n",
    "    y=[\n",
    "        \"fdm\",\n",
    "        \"fdm_30d_ema\",\n",
    "    ],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc786fcb",
   "metadata": {},
   "source": [
    "### Inspect Combined IDM * FDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2891e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_product_ser = idm_ser * fdm_ser\n",
    "dm_product_30d_ema = dm_product_ser.ewm(span=30, adjust=True, ignore_na=False).mean()\n",
    "# dm_product_30d_ema = idm_30d_ema * fdm_30d_ema\n",
    "\n",
    "df_tmp = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"idm\": idm_ser,\n",
    "            \"idm_30d_ema\": idm_30d_ema,\n",
    "            \"idm_trend\": idm_trend_ser,\n",
    "            \"idm_trend_30d_ema\": idm_trend_30d_ema,\n",
    "            \"dm_product\": dm_product_ser,\n",
    "            \"dm_product_30d_ema\": dm_product_30d_ema,\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"timestamp\"})\n",
    ")\n",
    "fig = px.line(\n",
    "    df_tmp,\n",
    "    x=\"timestamp\",\n",
    "    y=[\n",
    "        \"idm\",\n",
    "        \"idm_30d_ema\",\n",
    "        \"idm_trend\",\n",
    "        \"idm_trend_30d_ema\",\n",
    "        \"dm_product\",\n",
    "        \"dm_product_30d_ema\",\n",
    "    ],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8994f8",
   "metadata": {},
   "source": [
    "### Volatility Target Diversification Multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd33b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_target_to_realized = {\n",
    "    75.0: 48.11,\n",
    "    80.0: 49.93,\n",
    "    65.0: 44.7,\n",
    "    70.0: 46.59,\n",
    "    60.0: 42.72,\n",
    "    85.0: 51.41,\n",
    "    90.0: 52.77,\n",
    "    55.0: 40.59,\n",
    "    110.0: 57.52,\n",
    "    105.0: 56.67,\n",
    "    115.0: 58.4,\n",
    "    95.0: 54.02,\n",
    "    100.0: 55.49,\n",
    "    50.0: 38.31,\n",
    "    130.0: 59.88,\n",
    "    45.0: 35.86,\n",
    "    120.0: 59.2,\n",
    "    135.0: 60.7,\n",
    "    125.0: 59.96,\n",
    "    40.0: 33.19,\n",
    "    140.0: 61.64,\n",
    "    35.0: 29.97,\n",
    "    145.0: 62.05,\n",
    "    30.0: 26.29,\n",
    "    150.0: 62.13,\n",
    "    25.0: 22.22,\n",
    "    155.0: 62.61,\n",
    "    20.0: 18.1,\n",
    "    165.0: 63.3,\n",
    "    15.0: 13.76,\n",
    "    160.0: 62.67,\n",
    "    170.0: 63.17,\n",
    "    10.0: 9.2,\n",
    "    5.0: 4.62,\n",
    "    175.0: 63.3,\n",
    "    180.0: 64.2,\n",
    "    185.0: 64.24,\n",
    "    195.0: 64.88,\n",
    "    190.0: 64.4,\n",
    "    200.0: 64.68,\n",
    "}\n",
    "vol_target_to_scaling = {\n",
    "    k / 100: (k / v) for (k, v) in vol_target_to_realized.items() if k >= 0.0\n",
    "}\n",
    "x, y = zip(*vol_target_to_scaling.items())\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    trendline=\"lowess\",\n",
    "    trendline_options=dict(frac=0.4),\n",
    "    trendline_color_override=\"red\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b228828f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
